{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Hl9CxwwdGnqE",
        "UE-esM_NCGJ4",
        "7aOw3-a4GhoW",
        "u1rz83HWGtIM"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kfir1g/AI-Project/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiq3LhGCRiCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDcd_iqX3T-5",
        "colab_type": "text"
      },
      "source": [
        "#Initialize tensorflow-GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsOM9HBM2nSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "import tensorflow as tf "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoUX67u4AMfV",
        "colab_type": "text"
      },
      "source": [
        "##This part makes sure that we are working on GPU in runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2q4JizSKFbC",
        "colab_type": "code",
        "outputId": "c842cbe1-55b7-4d79-98e5-f2171c871ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at : {}'.format(device_name))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at : /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M83BQ3l232h",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2EglwEr6ptD",
        "colab_type": "text"
      },
      "source": [
        "##importing Libraries and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8eAaNYAAXlu",
        "colab_type": "text"
      },
      "source": [
        "##Import the data\n",
        "data is the results of each tree on each line of the testing data + the actual value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptPIzXyE1WDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"/content/gdrive/My Drive/AIProj/testing_by_tree_1_all.csv\") #Change to the path of data \n",
        "#dataset = pd.read_csv(\"/content/gdrive/My Drive/AIProj/All_trees_with_random_forst.csv\") #choose differnt file - choose the data which combined with random forest\n",
        "dataset = dataset.fillna(value=-32)#Min value for Nan values in the trees"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gqQzk5vxXoT",
        "colab_type": "text"
      },
      "source": [
        "#Getting new array for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlcKtLttrg0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "Sepdataset = pd.read_csv(\"/content/gdrive/My Drive/AIProj/All_trees_with_random_forest_September.csv\")\n",
        "Sepdataset = Sepdataset.fillna(value = -32) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpRrWJLery7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = Sepdataset.iloc[:,0:-1].values "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmvtfd4OsqPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X = sc_X.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upBJoMgW69Ra",
        "colab_type": "text"
      },
      "source": [
        "##Getting the dependent and independent variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYxgxwUHAw72",
        "colab_type": "text"
      },
      "source": [
        "splits the information to results from tree and actual result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF8DEI1s3IoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = dataset.iloc[:,1:-1].values \n",
        "y = dataset.iloc[:,-1].values\n",
        "myMin = -25 #Getting this min from the data, could be changed for differnt data \n",
        "y = y + myMin*-1 +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXz_mmclA9WM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toOqSp2fFyNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_PICKUP_BIKES = y.max() + 1 \n",
        "NUM_OF_INPUTS = X.shape[1]\n",
        "MAX_PICKUP_BIKES = 58 #Choose from the data, could be changed\n",
        "SHIFT = 26  # the min we decided + 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yds98lZrCpWJ",
        "colab_type": "text"
      },
      "source": [
        "##Splitting the dataset into training set and Test set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFytAK4A_yy",
        "colab_type": "text"
      },
      "source": [
        "Splits the information to training and test for the NN, test_size can be changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCdgLLE6Dm8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPoUIRX2zFv2",
        "colab_type": "text"
      },
      "source": [
        "##Random forest file split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zliObsqwzIYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train[:,0:-1]\n",
        "test_random_forest = X_test[:,-1]\n",
        "X_test = X_test[:,0:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zLnOPZzFBSP",
        "colab_type": "text"
      },
      "source": [
        "##Feature scaling - not needed for trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY_oW3QXCyz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "sc_Y = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl9CxwwdGnqE",
        "colab_type": "text"
      },
      "source": [
        "#Builed the NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE-esM_NCGJ4",
        "colab_type": "text"
      },
      "source": [
        "##Parameters for NN\n",
        "softmax - output neuron that got the highest value\n",
        "Epoch - how many time will go over all the data\n",
        "Batch_size - update weights every batch inputs\n",
        "Can add more layers or change the numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZPOMzTwIVP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIRST_LAYAR = 100\n",
        "SECONED_LAYAR = 60\n",
        "OUTPUT_ACTIVATION_FUNC = 'softmax'\n",
        "OPTIMIZER = 'adam'            \n",
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LkNjZ60E_-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(output_dim = FIRST_LAYAR  , init = 'uniform', activation = 'tanh', input_dim = NUM_OF_INPUTS))\n",
        "classifier.add(Dense(output_dim = SECONED_LAYAR, init = 'uniform', activation = 'tanh' ))\n",
        "classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aOw3-a4GhoW",
        "colab_type": "text"
      },
      "source": [
        "#Define Optimizer and Loss function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXnwRlgEMAW",
        "colab_type": "text"
      },
      "source": [
        "##Tuning the optimiers, and compiling the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnERn7meGWV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(lr=0.01, decay= 0, momentum=0.9, nesterov=True) #1e-6\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "rms = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
        "classifier.compile(optimizer = OPTIMIZER , loss = 'sparse_categorical_crossentropy',metrics = ['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rz83HWGtIM",
        "colab_type": "text"
      },
      "source": [
        "#Train the data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWfDQcyqEbwe",
        "colab_type": "text"
      },
      "source": [
        "##fit is the training, after that saves the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZxcKkgG0iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.fit(X_train, Y_train, batch_size = BATCH_SIZE , nb_epoch = NUM_EPOCH)\n",
        "from keras.models import load_model\n",
        "classifier.save_weights('/content/gdrive/My Drive/AIProj/my_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcKwA-p8FFbj",
        "colab_type": "text"
      },
      "source": [
        "#Load Wights \n",
        "To Load wights you need to create the model again and use the classifier to predict  - need to be tested"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcQ_SYsfFBHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "MAX_PICKUP_BIKES = 58 # y.max() +1 #Todo: check what is the max number\n",
        "NUM_OF_INPUTS = X_train.shape[1]\n",
        "FIRST_LAYAR = 100\n",
        "SECONED_LAYAR = 60\n",
        "OUTPUT_ACTIVATION_FUNC = 'softmax'\n",
        "OPTIMIZER = 'adam' #'adam' #stochastich gradient descent\n",
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 60\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(output_dim = FIRST_LAYAR  , init = 'uniform',activation ='tanh',input_dim = NUM_OF_INPUTS ))\n",
        "classifier.add(Dense(output_dim = SECONED_LAYAR, init = 'uniform', activation = 'tanh' ))\n",
        "classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        "classifier.load_weights('/content/gdrive/My Drive/AIProj/my_model_weights.h5') # Change to the path of the wights\n",
        "\n",
        "#How to predict one input: \n",
        "\n",
        "#input_vector = #put input vector you want to test\n",
        "#Y_pred = classifier.predict(input_vector)\n",
        "#pickup_predection = Y_pred[0].argmax()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLNRka6un5Be",
        "colab_type": "text"
      },
      "source": [
        "#Extract the predctions \n",
        "\n",
        "\n",
        "1.   getting the predection from the classifier\n",
        "2.   extracting the argmax which is the number of picups fo reach input\n",
        "3.  checking while using the test vector our results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEtSIllz1XL",
        "colab_type": "text"
      },
      "source": [
        "##Get predction for new matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKaTTjpDq5xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = classifier.predict(X)\n",
        "Y_get = []\n",
        "for i in range(len(Y_pred)):\n",
        "  Y_get.append(Y_pred[i].argmax() - SHIFT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHriHpm20Apc",
        "colab_type": "text"
      },
      "source": [
        "##adding the vector predction to the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su2Tlql1rdmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Sepdataset[\"demand\"] = Y_get\n",
        "Sepdataset.to_csv(\"/content/gdrive/My Drive/AIProj/Final.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j8wcGMe0Qj7",
        "colab_type": "text"
      },
      "source": [
        "##Extract results for our training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coVsoZZVHm22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = classifier.predict(X_test)\n",
        "Y_get = []\n",
        "for i in range(len(Y_pred)):\n",
        "  Y_get.append(Y_pred[i].argmax())\n",
        "a = np.asarray(Y_get)\n",
        "np.savetxt(\"/content/gdrive/My Drive/AIProj/Pred.csv\",a,delimiter = \",\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMdBpKxzITYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = 0 \n",
        "correct = 0 \n",
        "wrong = 0 \n",
        "histo_pred = []\n",
        "for i in range(len(Y_pred)):\n",
        "  total = total +1\n",
        "  if(Y_get[i] == Y_test[i]):\n",
        "     correct = correct + 1\n",
        "  else:\n",
        "    histo_pred.append(abs(Y_get[i] - Y_test[i] ))\n",
        "    wrong = wrong + 1\n",
        "\n",
        "    \n",
        "print(\"Total \"+ str(total) )\n",
        "print(\"Correct \" + str(correct))\n",
        "print(\"Wrong \" + str(wrong))\n",
        "print(\"acc:\" + str(float(correct)/total) )\n",
        "print(max(histo_pred))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nplNerhN014d",
        "colab_type": "text"
      },
      "source": [
        "##Histogram for out test prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHRs0WGgV6H6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = pd.Series(histo_pred).value_counts()\n",
        "plt.bar(x=hist.index, height=hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebJeqKJt0Y9X",
        "colab_type": "text"
      },
      "source": [
        "##Extract results for random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdhGhia5nsCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = 0 \n",
        "correct = 0 \n",
        "wrong = 0 \n",
        "histo_random_forest = []\n",
        "for i in range(len(test_random_forest)):\n",
        "  total = total +1\n",
        "  if(test_random_forest[i] == Y_test[i]):\n",
        "     correct = correct + 1\n",
        "  else:\n",
        "    histo_random_forest.append(abs(test_random_forest[i] - Y_test[i] ))\n",
        "    wrong = wrong + 1\n",
        "\n",
        "    \n",
        "print(\"Total \"+ str(total) )\n",
        "print(\"Correct \" + str(correct))\n",
        "print(\"Wrong \" + str(wrong))\n",
        "print(\"acc:\" + str(float(correct)/total) )\n",
        "print(max(histo_random_forest))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UZ1A4jl1Kjt",
        "colab_type": "text"
      },
      "source": [
        "##Histogram for random_forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s_2DZmX1SwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = pd.Series(histo_random_forest).value_counts()\n",
        "plt.bar(x=hist.index, height=hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubcrHwhcdPiv",
        "colab_type": "text"
      },
      "source": [
        "#Evaluating the ANN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpYGtDaCeBxr",
        "colab_type": "text"
      },
      "source": [
        "##Using K-Fold Cross Validation \n",
        "The main idea is dividing to 10 folds and taking the mean and variance, the mean is the actual result of our ANN and the variance will indicate if we are overfitting\n",
        "NN needs to be the same as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1v8GVajdO8E",
        "colab_type": "code",
        "outputId": "e4473006-1f44-4c44-cd42-4cb641ed318c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "def build_classifier():\n",
        "  classifier = Sequential()\n",
        "\n",
        "  classifier.add(Dense(output_dim = FIRST_LAYAR  , init = 'uniform',activation ='tanh',input_dim = NUM_OF_INPUTS ))\n",
        "  classifier.add(Dense(output_dim = SECONED_LAYAR, init = 'uniform', activation = 'tanh' ))\n",
        "  classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        "  classifier.compile(optimizer = 'adam'  , loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = BATCH_SIZE, nb_epoch = NUM_EPOCH  )\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = Y_train, cv =10, n_jobs = -1)# cv num of folds , n_jobs =-1 using all the GPU\n",
        "#Accuracies is a vector with all the #folds results(in our case 10) \n",
        "#The real Result is the mean of this vector, also we need to check for hight or low variance to know if our results are good\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "variance, mean\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.002268920817533607, 0.6896752746592789)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA3ynqBQl94C",
        "colab_type": "text"
      },
      "source": [
        "##Tuning The ANN - choose the best param from all the options\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KzTSvH7l_-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "def build_classifier(optimizer,first, seconed ):\n",
        "  classifier = Sequential()\n",
        "\n",
        "  classifier.add(Dense(output_dim = first  , init = 'uniform',activation ='tanh',input_dim = NUM_OF_INPUTS ))\n",
        "  classifier.add(Dense(output_dim = seconed, init = 'uniform', activation = 'tanh' ))\n",
        "  classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        "  classifier.compile(optimizer = optimizer  , loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "parameters = {'batch_size': [25, 50,75, 150 ],'epochs': [25], 'optimizer' : ['adam', 'rmsprop','sgd'], 'first': [100,200], 'seconed':[100,200]}\n",
        "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv = 10)  \n",
        "grid_serach = grid_search.fit(X_train, Y_train)                     \n",
        "best_parameters = grid_serach.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters, best_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}