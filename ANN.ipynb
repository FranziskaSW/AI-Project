{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Hl9CxwwdGnqE",
        "UE-esM_NCGJ4",
        "7aOw3-a4GhoW",
        "u1rz83HWGtIM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kfir1g/AI-Project/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiq3LhGCRiCS",
        "colab_type": "code",
        "outputId": "899d33bb-cd96-4f9e-f2cf-1faf9ce3024c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDcd_iqX3T-5",
        "colab_type": "text"
      },
      "source": [
        "#Initialize tensorflow-GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsOM9HBM2nSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "import tensorflow as tf \n",
        "#test111"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoUX67u4AMfV",
        "colab_type": "text"
      },
      "source": [
        "##This part makes sure that we are working on GPU in runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2q4JizSKFbC",
        "colab_type": "code",
        "outputId": "c842cbe1-55b7-4d79-98e5-f2171c871ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at : {}'.format(device_name))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at : /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M83BQ3l232h",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2EglwEr6ptD",
        "colab_type": "text"
      },
      "source": [
        "##importing Libraries and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8eAaNYAAXlu",
        "colab_type": "text"
      },
      "source": [
        "##Import the data\n",
        "data is the results of each tree on each line of the testing data + the actual value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptPIzXyE1WDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# dataset = pd.read_csv(\"/content/gdrive/My Drive/AIProj/testing_by_tree_1_all.csv\")\n",
        "dataset = pd.read_csv(\"/content/gdrive/My Drive/AIProj/All_trees_with_random_forst.csv\")\n",
        "# dataset = pd.read_csv(\"/content/gdrive/My Drive/AIProj/test.csv\")\n",
        "dataset = dataset.fillna(value=-32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gqQzk5vxXoT",
        "colab_type": "text"
      },
      "source": [
        "#Getting new array for predection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlcKtLttrg0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "Sepdataset = pd.read_csv(\"/content/gdrive/My Drive/AIProj/All_trees_with_random_forest_September.csv\")\n",
        "Sepdataset = Sepdataset.fillna(value = -32) \n",
        "# Sepdataset.head(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpRrWJLery7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = Sepdataset.iloc[:,0:-1].values "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmvtfd4OsqPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d33562b-40a2-40a1-8947-0bc0845883fe"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X = sc_X.fit_transform(X)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10816, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upBJoMgW69Ra",
        "colab_type": "text"
      },
      "source": [
        "##Getting the dependent and independent variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYxgxwUHAw72",
        "colab_type": "text"
      },
      "source": [
        "splits the information to results from tree and actual result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF8DEI1s3IoS",
        "colab_type": "code",
        "outputId": "4a557517-db6c-4f45-991b-4c7e8ce100d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = dataset.iloc[:,1:-1].values \n",
        "y = dataset.iloc[:,-1].values\n",
        "myMin = -25\n",
        "y = y + myMin*-1 +1\n",
        "#X = X + myMin*-1 + 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXz_mmclA9WM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toOqSp2fFyNZ",
        "colab_type": "code",
        "outputId": "21f1c68b-824e-403c-da4e-d3d0ab1409c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_PICKUP_BIKES = y.max() + 1 \n",
        "NUM_OF_INPUTS = X.shape[1]\n",
        "MAX_PICKUP_BIKES = 58\n",
        "SHIFT = 26  # the min we decided + 1 "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yds98lZrCpWJ",
        "colab_type": "text"
      },
      "source": [
        "##Splitting the dataset into training set and Test set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFytAK4A_yy",
        "colab_type": "text"
      },
      "source": [
        "Splits the information to training and test for the NN, test_size can be changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCdgLLE6Dm8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPoUIRX2zFv2",
        "colab_type": "text"
      },
      "source": [
        "##Random forest file split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zliObsqwzIYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train[:,0:-1]\n",
        "test_random_forest = X_test[:,-1]\n",
        "X_test = X_test[:,0:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zLnOPZzFBSP",
        "colab_type": "text"
      },
      "source": [
        "##Feature scaling - not needed for trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY_oW3QXCyz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "sc_Y = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl9CxwwdGnqE",
        "colab_type": "text"
      },
      "source": [
        "#Builed the NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE-esM_NCGJ4",
        "colab_type": "text"
      },
      "source": [
        "##Parameters for NN\n",
        "softmax - output neuron that got the highest value\n",
        "Epoch - how many time will go over all the data\n",
        "Batch_size - update weights every batch inputs\n",
        "Can add more layers or change the numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZPOMzTwIVP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FIRST_LAYAR = 100\n",
        "SECONED_LAYAR = 60\n",
        "OUTPUT_ACTIVATION_FUNC = 'softmax'\n",
        "OPTIMIZER = 'adam' #'sgd'# 'adam' #'rmsprop' #stochastich gradient descent\n",
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 60"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "162SXMnADiCW",
        "colab_type": "text"
      },
      "source": [
        "First layer has to have num_of_inputs\n",
        "Last layer has to have that activation\n",
        "We can add more layers, by copying the add option and changing options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbaOiLGSshL5",
        "colab_type": "text"
      },
      "source": [
        "first run : \n",
        "FIRST_LAYAR = 100\n",
        "SECONED_LAYAR = 60\n",
        "OUTPUT_ACTIVATION_FUNC = 'softmax'\n",
        "OPTIMIZER = 'adam' #'sgd'# 'adam' #'rmsprop' #stochastich gradient descent\n",
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 60\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(output_dim = FIRST_LAYAR  , init = 'uniform', activation = 'tanh', input_dim = NUM_OF_INPUTS))\n",
        "classifier.add(Dense(output_dim = SECONED_LAYAR, init = 'uniform', activation = 'tanh' ))\n",
        "classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LkNjZ60E_-A",
        "colab_type": "code",
        "outputId": "50318156-e891-432f-c9f0-a1139ae8fe18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(output_dim = FIRST_LAYAR  , init = 'uniform', activation = 'tanh', input_dim = NUM_OF_INPUTS))\n",
        "classifier.add(Dense(output_dim = SECONED_LAYAR, init = 'uniform', activation = 'tanh' ))\n",
        "classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", input_dim=40, units=100, kernel_initializer=\"uniform\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", units=60, kernel_initializer=\"uniform\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=58, kernel_initializer=\"uniform\")`\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aOw3-a4GhoW",
        "colab_type": "text"
      },
      "source": [
        "#Define Optimizer and Loss function "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AXnwRlgEMAW",
        "colab_type": "text"
      },
      "source": [
        "Change the values to change the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnERn7meGWV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(lr=0.01, decay= 0, momentum=0.9, nesterov=True) #1e-6\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "rms = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
        "classifier.compile(optimizer = OPTIMIZER , loss = 'sparse_categorical_crossentropy',metrics = ['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rz83HWGtIM",
        "colab_type": "text"
      },
      "source": [
        "#Train the data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWfDQcyqEbwe",
        "colab_type": "text"
      },
      "source": [
        "fit is the training, after that saves the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZxcKkgG0iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.fit(X_train, Y_train, batch_size = BATCH_SIZE , nb_epoch = NUM_EPOCH)\n",
        "from keras.models import load_model\n",
        "classifier.save_weights('/content/gdrive/My Drive/AIProj/my_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcKwA-p8FFbj",
        "colab_type": "text"
      },
      "source": [
        "#Load Wights \n",
        "To Load wights you need to create the model again and use the classifier to predict  - need to be tested"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcQ_SYsfFBHw",
        "colab_type": "code",
        "outputId": "6658e5d3-43f1-42f0-deb0-0c8a1d273827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "\n",
        "MAX_PICKUP_BIKES = 58 # y.max() +1 #Todo: check what is the max number\n",
        "NUM_OF_INPUTS = X.shape[1]\n",
        "FIRST_LAYAR = 100\n",
        "SECONED_LAYAR = 60\n",
        "OUTPUT_ACTIVATION_FUNC = 'softmax'\n",
        "OPTIMIZER = 'adam' #'adam' #stochastich gradient descent\n",
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 60\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "classifier = Sequential()\n",
        "\n",
        "classifier.add(Dense(output_dim = FIRST_LAYAR  , init = 'uniform',activation ='tanh',input_dim = NUM_OF_INPUTS ))\n",
        "classifier.add(Dense(output_dim = SECONED_LAYAR, init = 'uniform', activation = 'tanh' ))\n",
        "classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        "classifier.load_weights('/content/gdrive/My Drive/AIProj/my_model_weights.h5')\n",
        "\n",
        "#How to predict one input: \n",
        "\n",
        "#input_vector = #put input vector you want to test\n",
        "#Y_pred = classifier.predict(input_vector)\n",
        "#pickup_predection = Y_pred[0].argmax()\n"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", input_dim=40, units=100, kernel_initializer=\"uniform\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", units=60, kernel_initializer=\"uniform\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=58, kernel_initializer=\"uniform\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLNRka6un5Be",
        "colab_type": "text"
      },
      "source": [
        "#Extract the predctions \n",
        "\n",
        "\n",
        "1.   getting the predection from the classifier\n",
        "2.   extracting the argmax which is the number of picups fo reach input\n",
        "3.  checking while using the test vector our results\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEtSIllz1XL",
        "colab_type": "text"
      },
      "source": [
        "##Get predction for new matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKaTTjpDq5xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = classifier.predict(X)\n",
        "Y_get = []\n",
        "for i in range(len(Y_pred)):\n",
        "  Y_get.append(Y_pred[i].argmax() - SHIFT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHriHpm20Apc",
        "colab_type": "text"
      },
      "source": [
        "##adding the vector predction to the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su2Tlql1rdmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.array(Y_get)\n",
        "Sepdataset[\"demand\"] = Y_get\n",
        "Sepdataset.to_csv(\"/content/gdrive/My Drive/AIProj/Final.csv\")\n",
        "# Sepdataset.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j8wcGMe0Qj7",
        "colab_type": "text"
      },
      "source": [
        "##Extract results for our training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coVsoZZVHm22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = classifier.predict(X_test)\n",
        "Y_get = []\n",
        "for i in range(len(Y_pred)):\n",
        "  Y_get.append(Y_pred[i].argmax())\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMdBpKxzITYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = 0 \n",
        "correct = 0 \n",
        "wrong = 0 \n",
        "histo_pred = []\n",
        "for i in range(len(Y_pred)):\n",
        "  total = total +1\n",
        "  if(Y_get[i] == Y_test[i]):\n",
        "     correct = correct + 1\n",
        "  else:\n",
        "    histo_pred.append(abs(Y_get[i] - Y_test[i] ))\n",
        "    wrong = wrong + 1\n",
        "\n",
        "    \n",
        "print(\"Total \"+ str(total) )\n",
        "print(\"Correct \" + str(correct))\n",
        "print(\"Wrong \" + str(wrong))\n",
        "print(\"acc:\" + str(float(correct)/total) )\n",
        "print(max(histo_pred))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nplNerhN014d",
        "colab_type": "text"
      },
      "source": [
        "##Histogram for out test prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHRs0WGgV6H6",
        "colab_type": "code",
        "outputId": "28ba5404-3242-42d3-934f-ba4504c29cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "hist = pd.Series(histo_pred).value_counts()\n",
        "plt.bar(x=hist.index, height=hist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 20 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEnqGA6SpI7hIEnqGA6SpM7/AWs5vX7cLlu2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebJeqKJt0Y9X",
        "colab_type": "text"
      },
      "source": [
        "##Extract results for random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdhGhia5nsCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total = 0 \n",
        "correct = 0 \n",
        "wrong = 0 \n",
        "histo_random_forest = []\n",
        "for i in range(len(test_random_forest)):\n",
        "  total = total +1\n",
        "  if(test_random_forest[i] == Y_test[i]):\n",
        "     correct = correct + 1\n",
        "  else:\n",
        "    histo_random_forest.append(abs(test_random_forest[i] - Y_test[i] ))\n",
        "    wrong = wrong + 1\n",
        "\n",
        "    \n",
        "print(\"Total \"+ str(total) )\n",
        "print(\"Correct \" + str(correct))\n",
        "print(\"Wrong \" + str(wrong))\n",
        "print(\"acc:\" + str(float(correct)/total) )\n",
        "print(max(histo_random_forest))    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UZ1A4jl1Kjt",
        "colab_type": "text"
      },
      "source": [
        "##Histogram for random_forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s_2DZmX1SwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = pd.Series(histo_random_forest).value_counts()\n",
        "plt.bar(x=hist.index, height=hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubcrHwhcdPiv",
        "colab_type": "text"
      },
      "source": [
        "#Evaluating the ANN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpYGtDaCeBxr",
        "colab_type": "text"
      },
      "source": [
        "##Using K-Fold Cross Validation \n",
        "The main idea is dividing to 10 folds and taking the mean and variance, the mean is the actual result of our ANN and the variance will indicate if we are overfitting\n",
        "NN needs to be the same as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1v8GVajdO8E",
        "colab_type": "code",
        "outputId": "e4473006-1f44-4c44-cd42-4cb641ed318c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "def build_classifier():\n",
        "  classifier = Sequential()\n",
        "\n",
        "  classifier.add(Dense(output_dim = FIRST_LAYAR  , init = 'uniform',activation ='tanh',input_dim = NUM_OF_INPUTS ))\n",
        "  classifier.add(Dense(output_dim = SECONED_LAYAR, init = 'uniform', activation = 'tanh' ))\n",
        "  classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        "  classifier.compile(optimizer = 'adam'  , loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = build_classifier, batch_size = BATCH_SIZE, nb_epoch = NUM_EPOCH  )\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = Y_train, cv =10, n_jobs = -1)# cv num of folds , n_jobs =-1 usingall the GPU\n",
        "#Accuracies is a vector with all the #folds results(in our case 10) \n",
        "#The real Result is the mean of this vector, also we need to check for hight or low variance to know if our results are good\n",
        "mean = accuracies.mean()\n",
        "variance = accuracies.std()\n",
        "variance, mean\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.002268920817533607, 0.6896752746592789)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4_ul6loKRfg",
        "colab_type": "code",
        "outputId": "e064b449-b9e0-43d7-f536-3cc872ce8e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mean"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32813215512885086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hef3LqejkP-S",
        "colab_type": "text"
      },
      "source": [
        "#Improving the ANN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO-KrJGSkkip",
        "colab_type": "text"
      },
      "source": [
        "##Droput Regularization to reduce overfitting if needed \n",
        "While Getting big variance, meaning the model was doing well on some and than really bad on others, so he is overfitting on the training.\n",
        "This startagy will help fixing it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA3ynqBQl94C",
        "colab_type": "text"
      },
      "source": [
        "##Tuning The ANN - choose the best param from all the options\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KzTSvH7l_-h",
        "colab_type": "code",
        "outputId": "5514db65-09ca-4cdf-a839-02e4024da255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "def build_classifier(optimizer,first, seconed ):\n",
        "  classifier = Sequential()\n",
        "\n",
        "  classifier.add(Dense(output_dim = first  , init = 'uniform',activation ='tanh',input_dim = NUM_OF_INPUTS ))\n",
        "  classifier.add(Dense(output_dim = seconed, init = 'uniform', activation = 'tanh' ))\n",
        "  classifier.add(Dense(output_dim = MAX_PICKUP_BIKES, init = 'uniform', activation = OUTPUT_ACTIVATION_FUNC ))\n",
        "  classifier.compile(optimizer = optimizer  , loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
        "  return classifier\n",
        "\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "parameters = {'batch_size': [25, 50,75, 150 ],'epochs': [25], 'optimizer' : ['adam', 'rmsprop','sgd'], 'first': [100,200], 'seconed':[100,200]}\n",
        "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv = 10)  \n",
        "grid_serach = grid_search.fit(X_train, Y_train)                     \n",
        "best_parameters = grid_serach.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "best_parameters, best_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6da84dc40212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'first'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seconed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mgrid_serach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mbest_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_serach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJNwmn34pZhZ",
        "colab_type": "code",
        "outputId": "71f1e4a4-fe1a-4acc-af0c-b3d6aaa65ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "best_parameters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dc31fafe3c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'best_parameters' is not defined"
          ]
        }
      ]
    }
  ]
}